# Local Document Q&A System

A local-first document question-answering system built with LlamaIndex and FastEmbed, featuring a Streamlit web interface. This tool allows businesses to query their documents locally without relying on cloud services.

## Features

- ğŸ  **Fully Local**: All processing happens on your machine - no cloud dependencies
- ğŸ“š **Document Processing**: Handles various document formats
- ğŸ” **Fast Embedding**: Uses FastEmbed for efficient document embedding
- ğŸ–¥ï¸ **Web Interface**: Clean Streamlit UI for easy interaction
- ğŸš€ **Quick Setup**: Simple installation and configuration

## Installation

1. Clone the repository:
```bash
git clone [your-repo-url]
cd [repo-name]
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

## Usage

1. Place your documents in the `docs` directory

2. Run the Streamlit interface:
```bash
python -m streamlit run src/app.py
```

3. Access the web interface at `http://localhost:8502`

## Project Structure

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py              # Streamlit web interface
â”‚   â”œâ”€â”€ document_loader.py  # Document loading utilities
â”‚   â””â”€â”€ index_manager.py    # LlamaIndex integration
â”œâ”€â”€ docs/                   # Place your documents here
â”œâ”€â”€ requirements.txt        # Project dependencies
â””â”€â”€ README.md              # This file
```

## Dependencies

- Python 3.8+
- LlamaIndex
- FastEmbed
- Streamlit
- Other dependencies listed in requirements.txt

## Local Development

1. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

2. Install development dependencies:
```bash
pip install -r requirements.txt
```

## License

MIT License - See LICENSE file for details 