# Local GPT Document Q&A

A privacy-focused document Q&A system that runs entirely offline using Ollama and LlamaIndex.

## Features

- ğŸ”’ 100% offline operation - no cloud dependencies
- ğŸ“š Support for PDF, TXT, and DOCX files
- ğŸ’¬ Interactive chat interface
- ğŸ” Contextual answers with source tracking
- âš¡ Fast local inference using Mistral model

## Prerequisites

1. Install [Ollama](https://ollama.ai/)
2. Pull the Mistral model:
   ```bash
   ollama pull mistral
   ```

## Setup

1. Create a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Create a `docs` folder and add your documents:
   ```bash
   mkdir docs
   # Add your PDF, TXT, or DOCX files to the docs folder
   ```

## Usage

1. Start the Q&A system:
   ```bash
   python src/main.py
   ```

2. Ask questions about your documents in natural language
3. Type 'exit' to quit

## Project Structure

```
.
â”œâ”€â”€ docs/               # Place your documents here
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py        # Entry point
â”‚   â”œâ”€â”€ document_loader.py    # Document loading and parsing
â”‚   â”œâ”€â”€ index_manager.py     # Vector index management
â”‚   â””â”€â”€ query_engine.py      # Query processing and response generation
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ README.md          # This file
```

## Future Enhancements

- [ ] Desktop GUI (Tauri/Electron)
- [ ] Source chunk highlighting
- [ ] Markdown export
- [ ] Auto-reload on document changes 